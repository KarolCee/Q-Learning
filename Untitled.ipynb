{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Q-Learning with Python!\n",
    "\n",
    "## Analogy - teaching a dog some tricks:\n",
    "* the Dog is an **AGENT** exposed to the **ENVIRONMENT** - the house\n",
    "* the situations the Dog encounters are **STATES** - the Dog can sit, stand etc.\n",
    "* to go from one state to another the Dog performs an **ACTION** - goes from standing to sitting etc.\n",
    "* after the transition/action the Dog may receive a **REWARD** or a **PENALTY** - give a treat as reward, say 'no' as penalty\n",
    "* the strategy of choosing an action given a state in expectation of better results is called the **POLICY**\n",
    "\n",
    "\n",
    "To sum up: in the Environment there is an Agent who can be in different States. To change the state it performs an Action, for which it can get a Reward or a Penalty based on the chosen Policy! To find an optimal strategy/policy we learn from the experience!\n",
    "\n",
    "\n",
    "## Example - Self-driving Cab\n",
    "\n",
    "The story:\n",
    "the Smartcab's job is to pick up the passenger at one location and drop them off in another. \n",
    "Here are a few things that we'd like our Smartcab to take care of:\n",
    "\n",
    "1. Drop off the passenger at the right location.\n",
    "2. Save passenger's time by taking minimum time possible to drop off\n",
    "3. Take care of passenger's safety and traffic rules\n",
    "\n",
    "We need to consider three things when modeling an RL (Reinforcement Learning) solution to this problem:\n",
    "<ol>\n",
    "<li> <h4> Rewards </h4>\n",
    "Since a correct dropoff is highly required we have to give it a big reward. If the driver wants to dropoff in the wrong location it should be penalized. The agent should get a little penalty for not making it to the destination after every time-step. Only a little penalty because we want the driver to reach late instead of making wrong moves trying to reach the destination as quickly as possible. </li>\n",
    "\n",
    "<li>\n",
    "<h4> States </h4>\n",
    "the <i> State Space </i> is a set of all possible states that agent could inhabit - it should contain only useful information to make Agent make right decisions. We can break our parking area into 5x5 grid - this gives us 25 possible locations for a cab. There are also 4 locations where we can pick up/drop off a passenger: R, G, Y, B. When we account for one additional passenger state of being in a taxi, we have also 5 locations for our passenger. So in total we have $25*5*4=500$ total possible states. </li>\n",
    "\n",
    "<li> <h4> Actions </h4>\n",
    "The agent may encounter one of the 500 states and it takes an action from the <i> Action Space</i>. The actions in our case may be picking up/dropping off the passenger or moving in any direction. So in total we have 6 possible actions - the Action Space looks like this: <ul> <li> south </li>\n",
    "<li> north </li>\n",
    "<li> east </li>\n",
    "<li> west </li>\n",
    "<li> pickup </li>\n",
    "<li> dropoff </li>\n",
    "    </ul> <br>\n",
    "However the taxi driver cannot perform certain actions in certain states due to walls. In enviroment's code we just provide a $(-1)$ penalty for every wall hit and the taxi won't move anywhere. </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('Taxi-v3').env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code> env </code> is a core gym interface. It contains some useful methods:\n",
    "* <code>env.reset</code>: resets the environment and returns a random initial state\n",
    "* <code>env.step(action)</code>: steps the environment by one time-stamp and returns: \n",
    "    1. Observation of the environment\n",
    "    1. Reward which tells if our actions was beneficial or not\n",
    "    1. Done - an indicator which tells if we have successfuly picked up and dropped off a passenger - also called an         **episode**\n",
    "    1. Info - additional info such as performance and latency for debugging purposes\n",
    "<br>    <br>\n",
    "* <code>env.render</code>: renders one frame from the environment (helpful in visualizing the environment)\n",
    "\n",
    "\n",
    "Here is the reminder of the problem from Gym docs:\n",
    "<i>\"There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\"</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "Action space: Discrete(6)\n",
      "State space: Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() #resets the environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(f'Action space: {env.action_space}')\n",
    "print(f'State space: {env.observation_space}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The pipe **|** represents a wall\n",
    "1. the **<font color=blue>Blue</font>** letter represents the current passenger pick-up location and the  **<font color=purple>Purple</font>** letter is the current destination\n",
    "1. The filled square represents the taxi. The **<font color=yellow>Yellow</font>** color indicates that the passenger isn't in the taxi and the **<font color=green>Green</font>** color indicates that the passenger is in the taxi\n",
    "\n",
    "\n",
    "We need to find a way to identify a state uniquely by assigning a unique number to every possible state and RL learns to choose an action number from 0-5:\n",
    "* 0-south\n",
    "* 1-north\n",
    "* 2-east\n",
    "* 3-west\n",
    "* 4-pickup\n",
    "* 5-dropoff\n",
    "\n",
    "\n",
    "Each state of all 500 states corresponds to an encoding of the taxi's location, the passenger's location, and the destination location. The RL will learn a **mapping of states to the optimal action** to perform in that state by **exploration**, i.e. the Agent explores the environment and takes actions based on rewards defined in the environment. **The optimal action for each state is the action that has the highest cumulative long-term reward**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  426\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n"
     ]
    }
   ],
   "source": [
    "# we can use coordinates to generate a number corresponding to a state between 0 and 499\n",
    "state = env.encode(4, 1, 1, 2) #taxi row, taxi column, passenger index, destination index\n",
    "print(\"State: \", state)\n",
    "\n",
    "#we can set the environment's state manually using that encoded number and see everything move around\n",
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reward table\n",
    "\n",
    "When the Environment is created, there is an initial **Reward table** that's also created, called **'P'**. It is a matrix that has states as rows and actions as columns, i.e. $states \\times actions$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 426, -1, False)],\n",
       " 1: [(1.0, 326, -1, False)],\n",
       " 2: [(1.0, 446, -1, False)],\n",
       " 3: [(1.0, 426, -1, False)],\n",
       " 4: [(1.0, 426, -10, False)],\n",
       " 5: [(1.0, 426, -10, False)]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's check the default reward values assigned to our initial encoded state\n",
    "env.P[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary above has following structure: <code> {action: [(probability, nextstate, reward, done)]} </code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to note here:\n",
    "1. the 0-5 keys corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at the current state\n",
    "1. in this env the <code> probability </code>  is always 1.0\n",
    "1. the <code> nextstate </code> is the state we would be in if we take the action at this index of the dict\n",
    "1. </code> done </code> tells if we have successfully dropped of the passenger in the right location - each successful dropoff is the end of an **episode**\n",
    "\n",
    "If our agent choose to explore and action that makes it go through the wall (which is impossible in the source code of this game) it will just keep getting -1 penalties which affects the long-term reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the problem without Reinforcement Learning!\n",
    "Let's use brute force instead of RL. Since we have <code> P </code> Table for our default rewards in each state we can have our taxi navigate just using that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an infinite loop which runs untill one passenger reaches one destination (one episode) or in other words if the reward received is 20. The <code> env.action_space.sample()</code> automatically selects one random action from the set of all possible actions. Let's see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps taken: 2768\n",
      "Penalties incurred: 931\n"
     ]
    }
   ],
   "source": [
    "env.s = state #let's set the environment to state chosen earlier\n",
    "\n",
    "epochs = 0\n",
    "penalties, rewards = 0, 0\n",
    "\n",
    "frames = [] #for animation\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    if reward == -10: #if we pick up/ drop off in the wrong place\n",
    "        penalties += 1\n",
    "        \n",
    "    # locate each rendered frame into a dict for animation\n",
    "    frames.append({\n",
    "        'frame' : env.render(mode='ansi'),\n",
    "        'state' : state,\n",
    "        'action' : action,\n",
    "        'reward' : reward\n",
    "    })\n",
    "    \n",
    "    epochs+=1\n",
    "    \n",
    "    \n",
    "print(f'Timestamps taken: {epochs}')\n",
    "print(f'Penalties incurred: {penalties}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 2768\n",
      "State : 410\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "# we can also show an animation of all taken steps\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f'Timestep: {i+1}')\n",
    "        print(f\"State : {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)\n",
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks fun but doesn't work well - the taxi just drives everywhere making a lot of wrong dropoffs to deliver just one passenger. This is because it doesn't learn from past experience. We can run this program over and over and it will never optimize. The Agent has no memory of which action was the best for each state which is exactly what Reinforcement Learning will do for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving the problem with Reinforcement Learning!\n",
    "Let's use a simple RL algorithm called Q-learning which will give our agent some memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "Q-learning lets agent use the environment's rewards to learn the best action to take in a given state. In our Taxi environemnt we have the P table that the agent will learn from. It works by receiving a reward for taking an action in the current state, then updating a **Q-value** to remember if this action was beneficial! <br> <br>\n",
    "**The values stored in the Q-table are called Q-values and they map to a (state, action) combination**. The Q-value for a particular (state, action) combination represents the <i>Quality</i> of an action taken from that state. Greater Q-values imply greater chances of getting greater rewards. For example when we have a situations when the taxi is at a passenger location it is highly likely that the Q-value for pickup action is higher than compared to other actions like dropoff or south. <br><br>\n",
    "Q-values are initialized to arbitrary values and as agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\n",
    "\n",
    "$Q(state,action) =  (1-\\alpha)*Q(state,action) + \\alpha * (reward + \\gamma * maxQ(next state, all\\space actions))$ <br> <br> \n",
    "where\n",
    "* $\\alpha\\in<0,1>$ is the learning rate. It's an extent to which our Q-values are being updated in each iteration\n",
    "* $\\gamma\\in<0,1>$ is the discount factor. It determines how much importance to give to future rewards. A high value for discount factor (close to 1) captures the long-term effective award, whereas a discount=0 makes our agent consider only immediate award, hence making it a greedy approach.\n",
    "\n",
    "\n",
    "But what's the meaning of the equation above? <br>\n",
    "We are updating ($=$) the Q-value of the agent's current state and action combination by first taking a weight ($1-\\alpha$) of the old Q-value and then adding the learned value. The learned value is a combination of reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action. We store these Q-values in the Q-table. The Q-table is a matrix that has a row for every state and a column for every action (like P table). It's first initialized to 0 and then values are updated after training. Despite having the same dimensions as the reward table, the purpose of Q-table is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's summarize the process:\n",
    "1. Initialize the Q-table by zeros\n",
    "1. Start exploring the actions: for each state select any among all possible actions to take from the current state S\n",
    "1. Travel to the next state S' as a result of an action a\n",
    "1. For all possible actions from S' select the one with the highest Q-value\n",
    "1. Update Q-table values using the aforementioned equation\n",
    "1. Set the next state as the current state\n",
    "1. If the goal state is reached then end and repeat the process.\n",
    "\n",
    "### Exploiting the learned values\n",
    "After enough random explorations of the action space, the Q-values tend to converge serving our agent as an action-value function which can be used to pick the most optimal actions from a given state. There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learnt Q-values). We want to prevent the actions from taking the same route and possibly overfitting, so there's an another parameter $\\epsilon$ to cater this during training. Instead of selecting the best learnt Q-value action, we will sometimes favor the further exploration of the action space. Greater $\\epsilon$ values result in episodes with more penalties which is obvious for exploring by making random choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#let's first initialize a q-table to 500 x 6 matrix of zeros\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create a training algorithm that will update the q-table as the agent explores the environment. <br>\n",
    "In the first part of <code> while not done </code>, we decide whether to pick a random action or to exploit the already computed Q-values. This is done by using the <code> epsilon </code> value and comparing it to the <code> random.uniform(0,1) </code> function, which returns an arbitrary value between 0 and 1. <br> <br>\n",
    "We execute the chosen action in the environment to obtain the <code> next_state </code> and the <code> reward </code>. After that we calculate the maximum Q-value for the actions in the <code> next_state </code> and with that we can update our Q-value to <code> new_q_value </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training Finished\n",
      "Wall time: 52.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import random\n",
    "\n",
    "#Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "#for plotting matrices\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1,100001):\n",
    "    state = env.reset()\n",
    "    \n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() #explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) #exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "\n",
    "        new_value = (1-alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties+=1\n",
    "\n",
    "        state = next_state\n",
    "        epochs+=1\n",
    "        \n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "        \n",
    "print('Training Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that the Q-table has been trained over 100,000 episodes, let's see what the Q-values are at some random state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "The action should be: north\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "some_state = 328\n",
    "env.s = some_state\n",
    "env.render()\n",
    "action_labels = ['south','north','east','west','pickup','dropoff']\n",
    "print(f\"The action should be: {action_labels[np.argmax(q_table[some_state])]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.40779626,  -2.27325184,  -2.40477972,  -2.35721024,\n",
       "       -10.42831394,  -9.77575853])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[some_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the agent\n",
    "Let's evaluate the performance of our agent. We don't need to explore the actions any more so now the next action is always selected using the best Q-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 (100 different passengers) episodes:\n",
      "Average timestamps per episode: 13.77\n",
      "Average penalties per episode: 0.0\n",
      "Average rewards per move: 0.5250544662309368\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_penalties, total_rewards = 0, 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset() #let's start from some random state\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "            \n",
    "        epochs+=1\n",
    "        \n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    \n",
    "print(f\"Results after {episodes} (100 different passengers) episodes:\")\n",
    "print(f\"Average timestamps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
    "print(f\"Average rewards per move: {total_rewards / (total_epochs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for evaluation\n",
    "* Average number of penalties per episode - the lower this value the better performance, ideally equal to 0\n",
    "* Average number of timestamps per episode - we want a small number of timestamps since we want our agent to find the shortest path to reach the destination\n",
    "* Average rewards per move - greater rewards mean that the agent is doing the right thing. That's why deciding rewards is crucial for RL. In our case both timestamps and penalties are negatively rewarded, a higher reward would mean in this case that the agent reaches the destination as quick as possible with the least penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters and optimization\n",
    "The values for $\\alpha, \\gamma, \\epsilon$ were mostly based on intuition but there are better approaches. <br>\n",
    "Ideally all three parameters should decrease over time:\n",
    "* $\\alpha$ should decrease as we continue to gain greater and greater knowledge base\n",
    "* $\\gamma$ should decrease as we get closer to the deadline, since our preference for near-term reward should increase, as we won't be able to explore for long enough to get the long-term reward.\n",
    "* $\\epsilon$ - as we develop our policy we have less need of exploration and we can exploit the partially trained algorithm, so as trials increase the epsilon should decrease\n",
    "\n",
    "<br>\n",
    "A simple way to programatically come up with the best set of values for hyperaparameters is to create a search function (like grid search) that selects the parameters that would result in the best reward/time_stamps ratio. The reason for choosing this ratio is to enable us to get maximum reward as quickly as possible. We may also want to track the number of penalties because this can also be a deciding factor (for example the taxi that violates the rules in cost of reaching the destination faster isn't a good idea)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Q-learning is one of the easiest RL algorithms. The problem with this algorithm is that once the number of states in the environment is very high, it becomes hard to implement it with Q-table since the size would become very large. <br> <br>\n",
    "There are different techniques that uses Deep Neural Networks instead of Q-table (Deep Reinforcement Learning). The neural net takes as a input the state information along with actions and learns to output the correct action over time. Deep Learning Techniques such as CNN are also used to interpret pixels on the screen and get information out of the game (for example scores) and then let the agent control the game. However RL algorithms are not limited to games only. It is used for making humanoid robots or develop general AI agents, that can perform multiple things with a single algorithm (like the same agent playing multiple Atari games). To measure the AI's intelligence across games and websites Open AI provides a platform called the Universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My solutions to the exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Turn this code into a module of functions that can use multiple environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner():\n",
    "    def __init__(self, environment, n_iter = 50000, alpha = 0.1, gamma = 0.6, epsilon = 0.1):\n",
    "        #initialize hyperparameters\n",
    "        self.env = environment\n",
    "        self.n_iter = n_iter #number of episodes used for training\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        #initialize q-table with zeros\n",
    "        self.q_table = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        \n",
    "    # to train the agent    \n",
    "    def fit(self):\n",
    "        for i in range(self.n_iter):\n",
    "            state = self.env.reset()\n",
    "            reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                if random.uniform(0, 1) < self.epsilon:\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(self.q_table[state])\n",
    "                    \n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "                \n",
    "                old_value = self.q_table[state, action]\n",
    "                next_max = np.max(self.q_table[next_state])\n",
    "                \n",
    "                new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n",
    "                self.q_table[state, action] = new_value\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "            if (i % 100 == 0 or i==self.n_iter-1):\n",
    "                clear_output(wait = True)\n",
    "                print(f'Training: Episode: {i}/{self.n_iter}')\n",
    "                \n",
    "        print('Training Finished')\n",
    "        \n",
    "    #to measure how agent performs    \n",
    "    def evaluate(self, n_episodes=100, printing_frames = False):\n",
    "        total_epochs, total_rewards = 0, 0\n",
    "        frames = [] #just for animating\n",
    "        for i in range(n_episodes):\n",
    "            state = self.env.reset() #let's start each episode from a random state\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = np.argmax(self.q_table[state])\n",
    "                state, reward, done, info = self.env.step(action)\n",
    "                #for animating\n",
    "                frames.append({\n",
    "                'frame' : env.render(mode='ansi'),\n",
    "                'state' : state,\n",
    "                'action' : action,\n",
    "                'reward' : reward\n",
    "                })\n",
    "                \n",
    "                total_rewards += reward\n",
    "                total_epochs += 1\n",
    "            if (i % 100 == 0 or i==self.n_iter-1):\n",
    "                clear_output(wait = True)\n",
    "                print(f'Evaluating: Episode: {i}/{n_episodes}')\n",
    "        print('Evaluating Finished')\n",
    "        if printing_frames:\n",
    "            print_frames(frames)\n",
    "        \n",
    "        return total_rewards/total_epochs #measures how quickly we get a maximum reward - an average reward per move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_env = gym.make('Taxi-v3').env\n",
    "qlearner = QLearner(new_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Episode: 49999/50000\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "qlearner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Episode: 900/1000\n",
      "Evaluating Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5990253559735018"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = qlearner.evaluate(1000)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement a grid search to discover the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'gamma' : [0.6, 0.3], 'alpha': [0.1, 0.3], 'epsilon' : [0.1, 0.2]},\n",
    "    {'n_iter' : [150000, 100000], 'alpha' : [0.1, 0.2]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearnGridSearch():\n",
    "    def __init__(self, env, model, param_grid):\n",
    "        self.param_grid = param_grid\n",
    "        self.env = env\n",
    "        self.model = model\n",
    "    def __combine__(self, list_of_values,list_of_keys, acc, combinations):\n",
    "        last = (len(list_of_values) == 1)\n",
    "        n = len(list_of_values[0])\n",
    "        for i in range(n):\n",
    "            item = acc.copy()\n",
    "            item[list_of_keys[0]] = list_of_values[0][i]\n",
    "            if last:\n",
    "                combinations.append(item)\n",
    "            else:\n",
    "                self.__combine__(list_of_values[1:],list_of_keys[1:], item, combinations)\n",
    "    #Return all possible combinations of provided hyperparameters\n",
    "    def __combine_hyperparameters__(self, param_grid):\n",
    "        combinations = []\n",
    "        for single_dict in param_grid:\n",
    "            self.__combine__(list(single_dict.values()), list(single_dict.keys()), {}, combinations)\n",
    "        return combinations\n",
    "    def fit(self, n_eval_episodes = 100):\n",
    "        scores = []\n",
    "        combinations = self.__combine_hyperparameters__(self.param_grid)\n",
    "        for combination in combinations:\n",
    "            q_learner = self.model(env, **combination)\n",
    "            q_learner.fit()\n",
    "            score = q_learner.evaluate(n_eval_episodes)\n",
    "            scores.append((score, combination))\n",
    "        self.best_score = max(scores)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = QLearnGridSearch(env, QLearner, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Episode: 0/100\n",
      "Evaluating Finished\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the hyperparameters that provide the best reward/time_stamps ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.640625, {'n_iter': 100000, 'alpha': 0.2})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's tackle another problem from the gym - <i>FrozenLake-v0</i>\n",
    "#### Documentation: \n",
    "<i>The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. \n",
    "<br> <br>\n",
    "SFFF       (S: starting point, safe) <br>\n",
    "FHFH       (F: frozen surface, safe) <br>\n",
    "FFFH       (H: hole, fall to your doom) <br>\n",
    "HFFG       (G: goal) <br>\n",
    "\n",
    "The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: 4\n",
      "State space: 16\n"
     ]
    }
   ],
   "source": [
    "print(f\"Action space: {env.action_space.n}\")\n",
    "print(f\"State space: {env.observation_space.n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 actions (left, down, right, up), and 16 states (16 different tiles we can stand on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "#dummy agent, let's see how the game works\n",
    "env.reset()\n",
    "for i in range(10):\n",
    "    random_action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(random_action)\n",
    "    env.render()\n",
    "    print(f\"Reward: {reward}\")\n",
    "    clear_output(wait=True)\n",
    "    sleep(0.2)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our Q learning model class that we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learner = QLearner(env, n_iter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Episode: 99999/100000\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "q_learner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Episode: 0/100\n",
      "Evaluating Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.009845288326300985"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learner.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our custom Grid Search and see if we can obtain a better score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'gamma' : [0.6, 0.3], 'alpha': [0.1, 0.3], 'epsilon' : [0.1, 0.2]},\n",
    "    {'n_iter' : [150000, 100000], 'alpha' : [0.1, 0.2]},\n",
    "]\n",
    "grid_search = QLearnGridSearch(env, QLearner, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Episode: 0/100\n",
      "Evaluating Finished\n"
     ]
    }
   ],
   "source": [
    "grid_search.fit() #let's search for the best hyperparameters combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.013117283950617283, {'gamma': 0.6, 'alpha': 0.3, 'epsilon': 0.1})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Episode: 49999/50000\n",
      "Training Finished\n"
     ]
    }
   ],
   "source": [
    "best_q_learner = QLearner(env, **grid_search.best_score[1])\n",
    "best_q_learner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Episode: 0/100\n",
      "Evaluating Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.008027522935779817"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_q_learner.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
